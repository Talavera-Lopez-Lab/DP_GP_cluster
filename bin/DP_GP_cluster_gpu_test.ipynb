{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook version of DP_GP_cluster.py\n",
    "- Created on: 242802\n",
    "- Last modified: 242902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DP_GP import plot as dp_plt\n",
    "from DP_GP import utils\n",
    "from DP_GP import core_gpu as core\n",
    "from DP_GP import cluster_tools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as nl\n",
    "import scipy\n",
    "import GPy\n",
    "\n",
    "# import standard library dependencies:\n",
    "import collections\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of count matrice(s) filepath\n",
    "gene_expression_matrix = ['../../data/unstim_WT_counts_df.txt']\n",
    "# filepath to save the results\n",
    "output_path_prefix = '../../data/' \n",
    "\n",
    "## Optional sampling Arguments\n",
    "# Maximum number of Gibbs sampling iterations. default=1000\n",
    "max_num_iterations = 1000\n",
    "# Take every sth sample during Gibbs iterations to ensure independence between samples. default=3\n",
    "# thinning_param\n",
    "s = 3\n",
    "# Specify the optimization technique used to update GP hyperparameters\n",
    "# lbfgsb = L-BFGS-B \n",
    "# fmin_tnc = truncated Newton algorithm \n",
    "# simplex = Nelder-Mead simplex \n",
    "# scg = stochastic conjugate gradient \n",
    "# default = 'lbfgsb'\n",
    "optimizer = 'lbfgsb'\n",
    "# default=1000\n",
    "max_iters = 1000\n",
    "alpha = 1\n",
    "m = 4\n",
    "num_empty_clusters = 4\n",
    "fast = False\n",
    "check_convergence = False\n",
    "check_burnin_convergence = False\n",
    "sparse_regression = False\n",
    "criterion = 'MAP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional hyperprior arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma_n2_shape is shape parameter for the inverse gamma prior on the cluster noise variance. default=12\n",
    "sigma_n2_shape = 12\n",
    "# sigma_n2_rate is rate parameter for the inverse gamma prior on the cluster noise variance. default=2\n",
    "sigma_n2_rate = 2\n",
    "# for length scale [default=0]\n",
    "length_scale_mu = 0\n",
    "# for length scale [default=1]\n",
    "length_scale_sigma = 1\n",
    "# for signal variance [default=0]\n",
    "sigma_f_mu = 0\n",
    "# for signal variance [default=1]\n",
    "sigma_f_sigma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional post-processing arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process = False\n",
    "sim_mat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional output arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = True\n",
    "plot_types = ['pdf']\n",
    "time_unit = None\n",
    "save_cluster_GPs = False\n",
    "save_residuals = False\n",
    "do_not_plot_sim_mat = False\n",
    "cluster_uncertainty_estimate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional input transformation arguments\n",
    "\n",
    "# Set this flag if the header contains true time values (e.g. 0, 0.5, 4, 8,...)\n",
    "# and it is desired that the covariance kernel recognizes the true\n",
    "# time spacing between sampling points, which need not be constant.\n",
    "# Otherwise, it is assumed that the sampling times are equally spaced, \n",
    "# or in other words, that the rate of change in expression is roughly equivalent\n",
    "# between all neighboring time points.\n",
    "# default=False\n",
    "true_times = False \n",
    "\n",
    "# Set this flag if you desire the gene expression data to be clustered\n",
    "# without scaling (do not divide by standard deviation).\n",
    "# default=False\n",
    "unscaled = False\n",
    "\n",
    "#Set this flag if you desire the gene expression data to be clustered\n",
    "#without mean-centering (do not subtract mean).\n",
    "#default=False\n",
    "do_not_mean_center = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_expression_matrix, gene_names, t, t_labels = \\\n",
    "core.read_gene_expression_matrices(gene_expression_matrix, \n",
    "                                   true_times, \n",
    "                                   unscaled, \n",
    "                                   do_not_mean_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take median of inverse gamma distribution to yield point\n",
    "# estimate of sigma_n\n",
    "sigma_n2_shape, sigma_n2_rate = sigma_n2_shape, sigma_n2_rate\n",
    "sigma_n = np.sqrt(1 / ((sigma_n2_shape + 1) * sigma_n2_rate))\n",
    "\n",
    "# scale t such that the mean time interval between sampling points is one unit\n",
    "# this ensures that initial parameters for length-scale and signal variance are reasonable\n",
    "t /= np.mean(np.diff(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first phase of burn-in, expression trajectories cluster under initial length-scale and sigma_n parameters.\n",
    "burnIn_phaseI = int(np.floor(max_num_iterations/5) * 1.2)\n",
    "# second phase of burn-in, clusters optimize their hyperparameters.\n",
    "burnIn_phaseII = burnIn_phaseI * 2 \n",
    "# after burnIn_phaseII, samples are taken from the posterior\n",
    "\n",
    "# epsilon for similarity matrix squared distance convergence\n",
    "# and  epsilon for posterior log likelihood convergence\n",
    "# only used if --check_convergence\n",
    "sq_dist_eps, post_eps  = 0.01, 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Gibbs Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not post_process:\n",
    "    print(\"Begin sampling\")\n",
    "    GS = core.gibbs_sampler(gene_expression_matrix,t, max_num_iterations, max_iters, \\\n",
    "                            optimizer, burnIn_phaseI, burnIn_phaseII, alpha, m, \\\n",
    "                            s, check_convergence, check_burnin_convergence, sparse_regression, fast, \\\n",
    "                            sigma_n, sigma_n2_shape, sigma_n2_rate, \\\n",
    "                            length_scale_mu, length_scale_sigma, sigma_f_mu, \\\n",
    "                            sigma_f_sigma, sq_dist_eps, post_eps)\n",
    "    sim_mat, all_clusterings, sampled_clusterings, log_likelihoods, iter_num = GS.sampler()\n",
    "    \n",
    "    sampled_clusterings.columns = gene_names\n",
    "    all_clusterings.columns = gene_names\n",
    "else:\n",
    "    iter_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if criterion == 'MPEAR':\n",
    "    optimal_clusters = cluster_tools.best_clustering_by_mpear(np.array(sampled_clusterings), sim_mat)\n",
    "elif criterion == 'MAP':\n",
    "    optimal_clusters = cluster_tools.best_clustering_by_log_likelihood(np.array(sampled_clusterings), log_likelihoods)\n",
    "elif criterion == 'least_squares':\n",
    "    optimal_clusters = cluster_tools.best_clustering_by_sq_dist(np.array(sampled_clusterings), sim_mat)\n",
    "elif criterion == 'h_clust_avg':\n",
    "    optimal_clusters = cluster_tools.best_clustering_by_h_clust(sim_mat, 'average')\n",
    "elif criterion == 'h_clust_comp':\n",
    "    optimal_clusters = cluster_tools.best_clustering_by_h_clust(sim_mat, 'complete')\n",
    "\n",
    "# Given an optimal clustering, optimize the hyperparameters once again\n",
    "# because (1) hyperparameters are re-written at every iteration and (2) the particular clustering\n",
    "# may never have actually occurred during sampling, as may happen for h_clust_avg/h_clust_comp.\n",
    "\n",
    "optimal_cluster_labels = collections.defaultdict(list)\n",
    "optimal_cluster_labels_original_gene_names = collections.defaultdict(list)\n",
    "for gene, (gene_name, cluster) in enumerate(zip(gene_names, optimal_clusters)):\n",
    "    optimal_cluster_labels[cluster].append(gene)\n",
    "    optimal_cluster_labels_original_gene_names[cluster].append(gene_name)\n",
    "\n",
    "if cluster_uncertainty_estimate:\n",
    "    gene_to_prob = {}\n",
    "    for gene_i_k, (gene_name, cluster) in enumerate(zip(gene_names, optimal_clusters)):\n",
    "        genes_in_cluster = set(optimal_cluster_labels[cluster])\n",
    "        genes_j_k = genes_in_cluster - set([gene_i_k])\n",
    "        if len(genes_j_k) > 0:\n",
    "            gene_to_prob[gene_name] = sum([sim_mat[gene_i_k,gene_j_k] for gene_j_k in genes_j_k])/len(genes_j_k)\n",
    "        else:\n",
    "            gene_to_prob[gene_name] = 1.\n",
    "            \n",
    "\n",
    "if cluster_uncertainty_estimate:\n",
    "    print(\"Estimating cluster probability for each gene, loop:\", end=' ')\n",
    "    uncertainty_converged,last_gene_to_prob,prob_eps_cutoff,c,c_max=False,False,1e-8,0,200\n",
    "    while uncertainty_converged == False:\n",
    "        print(c, end=' ')\n",
    "        gene_to_prob = {}\n",
    "        for gene_i_k, (gene_name, cluster) in enumerate(zip(gene_names, optimal_clusters)):\n",
    "            genes_in_cluster = set(optimal_cluster_labels[cluster])\n",
    "            genes_j_k = genes_in_cluster - set([gene_i_k])\n",
    "            if len(genes_j_k) > 0:\n",
    "                if last_gene_to_prob is False:\n",
    "                    # first loop through iterative process estimates the\n",
    "                    # probability that gene belongs to cluster by taking\n",
    "                    # the mean proportion of times gene co-clusters\n",
    "                    # with every other gene in cluster\n",
    "                    denominator = len(genes_j_k)\n",
    "                    gene_to_prob[gene_i_k] = sum([sim_mat[gene_i_k,gene_j_k] for gene_j_k in genes_j_k])/denominator\n",
    "                else:\n",
    "                    # in subsequent loops, genes are weighted by how\n",
    "                    # likely they are to belong to a cluster. In this way,\n",
    "                    # the likelihood that a gene i belongs to cluster k \n",
    "                    # depends less on a gene j that is unlikely to belong to cluster k\n",
    "                    # and depends more on gene l that is likely to belong to cluster k\n",
    "                    denominator = sum([last_gene_to_prob[gene_j_k] for gene_j_k in genes_j_k])\n",
    "                    gene_to_prob[gene_i_k] = sum([sim_mat[gene_i_k,gene_j_k] * last_gene_to_prob[gene_j_k] for gene_j_k in genes_j_k])/denominator\n",
    "            else:\n",
    "                gene_to_prob[gene_i_k] = 1.\n",
    "                \n",
    "        if last_gene_to_prob is not False:\n",
    "            # find overall sum in absolute change in probability estimates\n",
    "            prob_eps = sum([np.abs(gene_to_prob[g] - last_gene_to_prob[g]) for g in sorted(gene_to_prob)])    \n",
    "            # check for convergence\n",
    "            if prob_eps < prob_eps_cutoff:\n",
    "                print(\"converged\")\n",
    "                for gene_i_k, gene_name in enumerate(gene_names):\n",
    "                    gene_to_prob[gene_name] = gene_to_prob[gene_i_k]\n",
    "                    del gene_to_prob[gene_i_k]\n",
    "                    \n",
    "                break\n",
    "        \n",
    "        last_gene_to_prob = gene_to_prob.copy()\n",
    "        c+=1\n",
    "        if c > c_max:\n",
    "            print(\"WARNING: iterative cluster_uncertainty_estimate did not converge\")\n",
    "            for gene_i_k, gene_name in enumerate(zip(gene_names)):\n",
    "                gene_to_prob[gene_name] = \"NA\"\n",
    "            break\n",
    "\n",
    "if save_residuals:\n",
    "    name_d = {gene:gene_name for gene, gene_name in enumerate(gene_names)}\n",
    "    residuals_by_gene = {}\n",
    "    \n",
    "optimal_clusters_GP = {}\n",
    "print(\"Optimizing parameters for optimal clusters.\")\n",
    "for cluster, genes in optimal_cluster_labels.items():\n",
    "    print(\"Cluster %s, %s genes\"%(cluster, len(genes)))\n",
    "    optimal_clusters_GP[cluster] = core.dp_cluster(members=genes, \n",
    "                                                   sigma_n=sigma_n, \n",
    "                                                   X=np.vstack(t), \n",
    "                                                   Y=np.array(np.mat(gene_expression_matrix[genes,:])).T, \n",
    "                                                   iter_num_at_birth=iter_num)\n",
    "    optimal_clusters_GP[cluster] = optimal_clusters_GP[cluster].update_cluster_attributes(gene_expression_matrix, \n",
    "                                                                                          sigma_n2_shape, \n",
    "                                                                                          sigma_n2_rate, \n",
    "                                                                                          length_scale_mu, \n",
    "                                                                                          length_scale_sigma, \n",
    "                                                                                          sigma_f_mu, \n",
    "                                                                                          sigma_f_sigma, \n",
    "                                                                                          iter_num, \n",
    "                                                                                          max_iters, \n",
    "                                                                                          optimizer)\n",
    "    if save_residuals:\n",
    "        for gene in genes:\n",
    "            resids = ( gene_expression_matrix[gene,:] - optimal_clusters_GP[cluster].mean )**2\n",
    "            residuals_by_gene[name_d[gene]] = resids\n",
    "            \n",
    "\n",
    "if save_residuals:\n",
    "    residuals_df = pd.DataFrame(np.array([residuals_by_gene[gene_name] for gene_name in gene_names]))\n",
    "    residuals_df.columns = t_labels\n",
    "    residuals_df.index = gene_names\n",
    "    residuals_df.to_csv(output_path_prefix + \"_residuals.txt\", sep='\\t', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not post_process:\n",
    "    print(\"Saving sampling results.\")\n",
    "    core.save_posterior_similarity_matrix(sim_mat, gene_names, output_path_prefix)\n",
    "    core.save_clusterings(sampled_clusterings, output_path_prefix)\n",
    "    core.save_log_likelihoods(log_likelihoods, output_path_prefix)\n",
    "\n",
    "if not cluster_uncertainty_estimate:\n",
    "    cluster_tools.save_cluster_membership_information(optimal_cluster_labels_original_gene_names, \n",
    "                                                      output_path_prefix + \"_optimal_clustering.txt\")\n",
    "else:\n",
    "    cluster_tools.save_cluster_membership_information(optimal_cluster_labels_original_gene_names, \n",
    "                                                      output_path_prefix + \"_optimal_clustering.txt\",\n",
    "                                                      gene_to_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    print(\"Plotting expression and sampling results.\")    \n",
    "    plot_types = plot_types\n",
    "    if not post_process or sim_mat and not do_not_plot_sim_mat:\n",
    "        try:\n",
    "            sim_mat_key = dp_plt.plot_similarity_matrix(\n",
    "                sim_mat, output_path_prefix, plot_types\n",
    "                )\n",
    "        except RuntimeError:\n",
    "            print(\"WARNING: skipping heatmap plot generation, too many dendrogram recursions for scipy to handle\")\n",
    "        \n",
    "    if not post_process:    \n",
    "        core.save_posterior_similarity_matrix_key(\n",
    "            [gene_names[idx] for idx in sim_mat_key], \n",
    "            output_path_prefix\n",
    "            )\n",
    "        dp_plt.plot_cluster_sizes_over_iterations(\n",
    "            np.array(all_clusterings), \n",
    "            burnIn_phaseI, \n",
    "            burnIn_phaseII, \n",
    "            m, \n",
    "            output_path_prefix, \n",
    "            plot_types\n",
    "            )\n",
    "    \n",
    "    dp_plt.plot_cluster_gene_expression(\n",
    "       optimal_clusters_GP,\n",
    "       pd.DataFrame(\n",
    "           gene_expression_matrix, index=gene_names, columns=t),\n",
    "       t,\n",
    "       t_labels,\n",
    "       time_unit,\n",
    "       output_path_prefix,\n",
    "       plot_types, \n",
    "       unscaled,\n",
    "       do_not_mean_center,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_cluster_GPs:    \n",
    "    param_df = pd.DataFrame({name:dp_cluster.model.param_array for name, dp_cluster in optimal_clusters_GP.items()}) \n",
    "    param_df.index = dp_cluster.model.parameter_names()\n",
    "    param_df.to_csv(output_path_prefix + \"_cluster_model_params.txt\", sep='\\t', index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP_GP-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
